{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/wenger/zhangjf/ATAE-LSTM/data/Embedding.ipynb\n",
      "importing Jupyter notebook from /home/wenger/zhangjf/ATAE-LSTM/data/AspClas.ipynb\n",
      "importing Jupyter notebook from /home/wenger/zhangjf/ATAE-LSTM/models/ATAE_LSTM.ipynb\n",
      "importing Jupyter notebook from /home/wenger/zhangjf/ATAE-LSTM/models/BasicModule.ipynb\n"
     ]
    }
   ],
   "source": [
    "import ipdb\n",
    "import sys\n",
    "sys.path.append('../ATAE-LSTM')\n",
    "import Ipynb_importer\n",
    "from data.Embedding import Emb\n",
    "from data.AspClas import AspClas\n",
    "from models.ATAE_LSTM import ATAE_LSTM\n",
    "from utils.visualize import Visualizer\n",
    "from config import opt\n",
    "from tqdm import tqdm\n",
    "from random import randint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch.utils.data import DataLoader\n",
    "from torchnet import meter\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, dataloader):\n",
    "    '''\n",
    "    计算模型在验证集上的准确率等信息\n",
    "    '''\n",
    "    \n",
    "    confusion_matrix = meter.ConfusionMeter(opt.classes)\n",
    "    model.eval()\n",
    "    with t.no_grad():\n",
    "        for step, data in enumerate(dataloader):\n",
    "            sentence, terms, label = data\n",
    "            if opt.use_cuda:\n",
    "                sentence, terms, label = sentence.cuda(), terms.cuda(), label.cuda()\n",
    "            score = model(sentence, terms)\n",
    "            confusion_matrix.add(score.data.cpu(), label.data.cpu().squeeze())\n",
    "    model.train()\n",
    "    cm_value = confusion_matrix.value()\n",
    "    accuracy = 100.0 * (cm_value.trace()) / (cm_value.sum())\n",
    "    class_equal_accuracy = (float(cm_value[0][0])/(cm_value[0].sum())\n",
    "        +float(cm_value[1][1])/(cm_value[1].sum())\n",
    "        +float(cm_value[2][2])/(cm_value[2].sum())\n",
    "    ) * 100 / 3\n",
    "    return confusion_matrix, accuracy, class_equal_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "#! python -m visdom.server\n",
    "vis = Visualizer(opt.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3044/3044 [00:00<00:00, 467039.59it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 274403.31it/s]\n",
      "100%|██████████| 100000/100000 [00:06<00:00, 14510.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding : successfully input 100000 pretrained word embeddings while 0 failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3044/3044 [00:00<00:00, 687753.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# step1 data\n",
    "train_data = AspClas(opt.train_data_root, train=True)\n",
    "test_data = AspClas(opt.train_data_root, train=False, emb=train_data.emb)\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    opt.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last = True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    opt.batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last = True\n",
    ")\n",
    "words = train_data.emb._get_words_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2 configure model\n",
    "model = ATAE_LSTM(emb=train_data.emb)\n",
    "if opt.use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3 criterion and optimizer\n",
    "if opt.rescaling:\n",
    "    class_weights = t.Tensor([3,4,1]) # weights on every class\n",
    "    if opt.use_cuda:\n",
    "        class_weights = class_weights.cuda()\n",
    "else:\n",
    "    class_weights = None\n",
    "criterion = t.nn.CrossEntropyLoss(weight = class_weights)\n",
    "lr = opt.lr\n",
    "optimizer = t.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = lr,\n",
    "    weight_decay = opt.weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step4 meters\n",
    "loss_meter = meter.AverageValueMeter()\n",
    "confusion_matrix = meter.ConfusionMeter(opt.classes)\n",
    "previous_loss = 1e100\n",
    "best_val_accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attention(model, words, test_dataloader=None, sentence_terms_label=None):\n",
    "    if test_dataloader is not None:\n",
    "        sentence, terms, label = list(test_dataloader)[0]\n",
    "        if opt.use_cuda:\n",
    "            sentence, terms, label = sentence.cuda(), terms.cuda(), label.cuda()\n",
    "    else:\n",
    "        (sentence, terms, label) = sentence_terms_label\n",
    "    tokens = [words[i] for i in sentence[0].tolist() if i!=0]\n",
    "    term = [words[i] for i in terms[0].tolist() if i!=0]\n",
    "    score, attention = model(sentence, terms, returnAttention=True)\n",
    "    attention_probs = attention[0][0][:len(tokens)].tolist()\n",
    "    tokens_attention = [(tokens[i], \"%.3f\"%attention_probs[i]) for i in range(len(tokens))]\n",
    "    tqdm.write(str(tokens))\n",
    "    tqdm.write(str(term))\n",
    "    tqdm.write(str(tokens_attention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [00:03<30:55,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'ambience', 'is', 'very', 'romantic', 'and', 'definitely', 'a', 'good', 'place', 'to', 'bring', 'a', 'date', '.']\n",
      "['place']\n",
      "[('the', '0.035'), ('ambience', '0.032'), ('is', '0.060'), ('very', '0.064'), ('romantic', '0.049'), ('and', '0.091'), ('definitely', '0.059'), ('a', '0.079'), ('good', '0.078'), ('place', '0.075'), ('to', '0.090'), ('bring', '0.072'), ('a', '0.084'), ('date', '0.069'), ('.', '0.062')]\n",
      "['i', 'particularly', 'love', 'their', 'yellowfun', 'tuna', 'and', 'their', 'mussel', 'selection', '.']\n",
      "['mussel', 'selection']\n",
      "[('i', '0.048'), ('particularly', '0.051'), ('love', '0.084'), ('their', '0.115'), ('yellowfun', '0.085'), ('tuna', '0.058'), ('and', '0.125'), ('their', '0.137'), ('mussel', '0.076'), ('selection', '0.115'), ('.', '0.106')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 51/500 [03:06<28:21,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'ambience', 'is', 'very', 'romantic', 'and', 'definitely', 'a', 'good', 'place', 'to', 'bring', 'a', 'date', '.']\n",
      "['place']\n",
      "[('the', '0.137'), ('ambience', '0.124'), ('is', '0.074'), ('very', '0.060'), ('romantic', '0.070'), ('and', '0.059'), ('definitely', '0.057'), ('a', '0.060'), ('good', '0.053'), ('place', '0.051'), ('to', '0.049'), ('bring', '0.056'), ('a', '0.060'), ('date', '0.044'), ('.', '0.047')]\n",
      "['i', 'have', 'never', 'been', 'disappointed', 'but', 'their', 'true', 'strength', 'lays', 'in', 'their', 'amazingly', 'delicious', 'and', 'cheap', 'lunch', 'specials', '.']\n",
      "['lunch', 'specials']\n",
      "[('i', '0.064'), ('have', '0.045'), ('never', '0.041'), ('been', '0.037'), ('disappointed', '0.052'), ('but', '0.047'), ('their', '0.048'), ('true', '0.049'), ('strength', '0.059'), ('lays', '0.071'), ('in', '0.057'), ('their', '0.052'), ('amazingly', '0.059'), ('delicious', '0.049'), ('and', '0.052'), ('cheap', '0.060'), ('lunch', '0.050'), ('specials', '0.056'), ('.', '0.052')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 101/500 [06:13<25:09,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'ambience', 'is', 'very', 'romantic', 'and', 'definitely', 'a', 'good', 'place', 'to', 'bring', 'a', 'date', '.']\n",
      "['place']\n",
      "[('the', '0.057'), ('ambience', '0.069'), ('is', '0.064'), ('very', '0.066'), ('romantic', '0.095'), ('and', '0.097'), ('definitely', '0.069'), ('a', '0.056'), ('good', '0.063'), ('place', '0.058'), ('to', '0.065'), ('bring', '0.065'), ('a', '0.055'), ('date', '0.060'), ('.', '0.063')]\n",
      "['my', 'goodness', ',', 'everything', 'from', 'the', 'fish', 'to', 'the', 'rice', 'to', 'the', 'seaweed', 'was', 'absolutely', 'amazing', '.']\n",
      "['fish']\n",
      "[('my', '0.060'), ('goodness', '0.059'), (',', '0.060'), ('everything', '0.058'), ('from', '0.062'), ('the', '0.054'), ('fish', '0.058'), ('to', '0.062'), ('the', '0.053'), ('rice', '0.060'), ('to', '0.063'), ('the', '0.054'), ('seaweed', '0.064'), ('was', '0.062'), ('absolutely', '0.057'), ('amazing', '0.058'), ('.', '0.058')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 151/500 [09:21<22:00,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'ambience', 'is', 'very', 'romantic', 'and', 'definitely', 'a', 'good', 'place', 'to', 'bring', 'a', 'date', '.']\n",
      "['place']\n",
      "[('the', '0.011'), ('ambience', '0.020'), ('is', '0.020'), ('very', '0.119'), ('romantic', '0.174'), ('and', '0.092'), ('definitely', '0.088'), ('a', '0.056'), ('good', '0.212'), ('place', '0.088'), ('to', '0.027'), ('bring', '0.031'), ('a', '0.032'), ('date', '0.014'), ('.', '0.017')]\n",
      "['so', 'some', 'of', 'the', 'reviews', 'here', 'are', 'accurate', 'about', 'the', 'crowd', 'and', 'noise', '.']\n",
      "['noise']\n",
      "[('so', '0.106'), ('some', '0.060'), ('of', '0.023'), ('the', '0.056'), ('reviews', '0.063'), ('here', '0.047'), ('are', '0.042'), ('accurate', '0.135'), ('about', '0.061'), ('the', '0.084'), ('crowd', '0.110'), ('and', '0.044'), ('noise', '0.078'), ('.', '0.091')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 201/500 [12:29<18:55,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'ambience', 'is', 'very', 'romantic', 'and', 'definitely', 'a', 'good', 'place', 'to', 'bring', 'a', 'date', '.']\n",
      "['place']\n",
      "[('the', '0.007'), ('ambience', '0.067'), ('is', '0.009'), ('very', '0.135'), ('romantic', '0.241'), ('and', '0.073'), ('definitely', '0.087'), ('a', '0.045'), ('good', '0.232'), ('place', '0.071'), ('to', '0.005'), ('bring', '0.006'), ('a', '0.011'), ('date', '0.003'), ('.', '0.008')]\n",
      "['the', 'rice', 'to', 'fish', 'ration', 'was', 'also', 'good', '--', 'they', 'did', \"n't\", 'try', 'to', '<UNKNOWN>', 'the', 'rice', '.']\n",
      "['rice', 'to', 'fish', 'ration']\n",
      "[('the', '0.056'), ('rice', '0.043'), ('to', '0.013'), ('fish', '0.021'), ('ration', '0.047'), ('was', '0.072'), ('also', '0.039'), ('good', '0.145'), ('--', '0.080'), ('they', '0.065'), ('did', '0.089'), (\"n't\", '0.066'), ('try', '0.053'), ('to', '0.016'), ('<UNKNOWN>', '0.062'), ('the', '0.053'), ('rice', '0.042'), ('.', '0.038')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 251/500 [15:33<15:31,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'ambience', 'is', 'very', 'romantic', 'and', 'definitely', 'a', 'good', 'place', 'to', 'bring', 'a', 'date', '.']\n",
      "['place']\n",
      "[('the', '0.005'), ('ambience', '0.064'), ('is', '0.006'), ('very', '0.068'), ('romantic', '0.073'), ('and', '0.069'), ('definitely', '0.078'), ('a', '0.029'), ('good', '0.412'), ('place', '0.161'), ('to', '0.009'), ('bring', '0.011'), ('a', '0.008'), ('date', '0.003'), ('.', '0.005')]\n",
      "['however', ',', 'their', 'popularity', 'has', 'yet', 'to', 'slow', 'down', ',', 'and', 'i', 'still', 'find', 'myself', 'drawn', 'to', 'their', 'ambiance', 'and', 'delectable', 'reputation', '.']\n",
      "['ambiance']\n",
      "[('however', '0.013'), (',', '0.010'), ('their', '0.008'), ('popularity', '0.012'), ('has', '0.015'), ('yet', '0.014'), ('to', '0.015'), ('slow', '0.007'), ('down', '0.004'), (',', '0.005'), ('and', '0.020'), ('i', '0.053'), ('still', '0.021'), ('find', '0.040'), ('myself', '0.016'), ('drawn', '0.033'), ('to', '0.024'), ('their', '0.012'), ('ambiance', '0.022'), ('and', '0.046'), ('delectable', '0.274'), ('reputation', '0.237'), ('.', '0.099')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 301/500 [18:40<12:30,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'ambience', 'is', 'very', 'romantic', 'and', 'definitely', 'a', 'good', 'place', 'to', 'bring', 'a', 'date', '.']\n",
      "['place']\n",
      "[('the', '0.010'), ('ambience', '0.060'), ('is', '0.008'), ('very', '0.064'), ('romantic', '0.079'), ('and', '0.092'), ('definitely', '0.134'), ('a', '0.048'), ('good', '0.327'), ('place', '0.105'), ('to', '0.017'), ('bring', '0.022'), ('a', '0.014'), ('date', '0.006'), ('.', '0.012')]\n",
      "['to', 'begin', ',', 'we', 'were', 'told', 'there', 'was', 'a', '30', 'minute', 'wait', 'and', 'started', 'to', 'leave', ',', 'when', 'the', 'hostess', 'offered', 'to', 'call', 'us', 'on', 'our', 'cell', 'phone', 'when', 'the', 'table', 'was', 'ready', '.']\n",
      "['hostess']\n",
      "[('to', '0.011'), ('begin', '0.028'), (',', '0.015'), ('we', '0.016'), ('were', '0.061'), ('told', '0.096'), ('there', '0.047'), ('was', '0.086'), ('a', '0.036'), ('30', '0.019'), ('minute', '0.027'), ('wait', '0.040'), ('and', '0.021'), ('started', '0.081'), ('to', '0.030'), ('leave', '0.062'), (',', '0.020'), ('when', '0.016'), ('the', '0.007'), ('hostess', '0.026'), ('offered', '0.028'), ('to', '0.020'), ('call', '0.029'), ('us', '0.018'), ('on', '0.007'), ('our', '0.012'), ('cell', '0.018'), ('phone', '0.009'), ('when', '0.013'), ('the', '0.007'), ('table', '0.004'), ('was', '0.030'), ('ready', '0.030'), ('.', '0.028')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 351/500 [21:42<08:43,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'ambience', 'is', 'very', 'romantic', 'and', 'definitely', 'a', 'good', 'place', 'to', 'bring', 'a', 'date', '.']\n",
      "['place']\n",
      "[('the', '0.013'), ('ambience', '0.058'), ('is', '0.008'), ('very', '0.051'), ('romantic', '0.111'), ('and', '0.143'), ('definitely', '0.164'), ('a', '0.037'), ('good', '0.256'), ('place', '0.075'), ('to', '0.016'), ('bring', '0.027'), ('a', '0.011'), ('date', '0.008'), ('.', '0.019')]\n",
      "['we', 'have', 'never', 'had', 'any', 'problems', 'with', 'charging', 'the', 'meal', 'or', 'the', 'tip', ',', 'and', 'the', 'food', 'was', 'delivered', 'quickly', ',', 'but', 'we', 'live', 'only', 'a', 'few', 'minutes', 'walk', 'from', 'them', '.']\n",
      "['food']\n",
      "[('we', '0.004'), ('have', '0.001'), ('never', '0.003'), ('had', '0.002'), ('any', '0.000'), ('problems', '0.001'), ('with', '0.001'), ('charging', '0.026'), ('the', '0.004'), ('meal', '0.019'), ('or', '0.001'), ('the', '0.002'), ('tip', '0.007'), (',', '0.004'), ('and', '0.003'), ('the', '0.006'), ('food', '0.019'), ('was', '0.033'), ('delivered', '0.214'), ('quickly', '0.605'), (',', '0.014'), ('but', '0.003'), ('we', '0.002'), ('live', '0.010'), ('only', '0.001'), ('a', '0.003'), ('few', '0.002'), ('minutes', '0.003'), ('walk', '0.003'), ('from', '0.002'), ('them', '0.001'), ('.', '0.004')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 401/500 [24:45<06:18,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'ambience', 'is', 'very', 'romantic', 'and', 'definitely', 'a', 'good', 'place', 'to', 'bring', 'a', 'date', '.']\n",
      "['place']\n",
      "[('the', '0.014'), ('ambience', '0.086'), ('is', '0.007'), ('very', '0.035'), ('romantic', '0.180'), ('and', '0.090'), ('definitely', '0.199'), ('a', '0.043'), ('good', '0.231'), ('place', '0.046'), ('to', '0.009'), ('bring', '0.018'), ('a', '0.012'), ('date', '0.007'), ('.', '0.024')]\n",
      "['service', 'was', 'very', 'good', 'and', 'warm', '.']\n",
      "['service']\n",
      "[('service', '0.389'), ('was', '0.123'), ('very', '0.085'), ('good', '0.012'), ('and', '0.027'), ('warm', '0.306'), ('.', '0.059')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 451/500 [27:54<03:08,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'ambience', 'is', 'very', 'romantic', 'and', 'definitely', 'a', 'good', 'place', 'to', 'bring', 'a', 'date', '.']\n",
      "['place']\n",
      "[('the', '0.015'), ('ambience', '0.121'), ('is', '0.007'), ('very', '0.034'), ('romantic', '0.160'), ('and', '0.057'), ('definitely', '0.196'), ('a', '0.052'), ('good', '0.262'), ('place', '0.027'), ('to', '0.007'), ('bring', '0.014'), ('a', '0.015'), ('date', '0.008'), ('.', '0.026')]\n",
      "['try', 'the', 'crunchy', 'tuna', ',', 'it', 'is', 'to', 'die', 'for', '.']\n",
      "['crunchy', 'tuna']\n",
      "[('try', '0.064'), ('the', '0.050'), ('crunchy', '0.389'), ('tuna', '0.424'), (',', '0.014'), ('it', '0.030'), ('is', '0.006'), ('to', '0.001'), ('die', '0.017'), ('for', '0.001'), ('.', '0.003')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [30:59<00:00,  3.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# step5 train\n",
    "\n",
    "# validate and visualize at start\n",
    "val_cm, val_accuracy, class_equal_accuracy = val(model, test_dataloader)\n",
    "vis.plot('val_accuracy', val_accuracy)\n",
    "vis.plot('val_class_equal_accuracy', class_equal_accuracy)\n",
    "vis.plot('lr', lr)\n",
    "vis.log(\"epoch:{epoch},\\nlr:{lr},\\ntrain_cm:{train_cm},\\nval_cm:{val_cm}\".format(\n",
    "    epoch = 0,\n",
    "    val_cm = str(val_cm.value()),\n",
    "    train_cm=str(confusion_matrix.value()),\n",
    "    lr=lr\n",
    "))\n",
    "\n",
    "total_step = 0\n",
    "for epoch in tqdm(range(opt.max_epoch)):\n",
    "    loss_meter.reset()\n",
    "    confusion_matrix.reset()\n",
    "    \n",
    "    for step, (sentence, terms, label) in enumerate(train_dataloader):\n",
    "        \n",
    "        if opt.use_cuda:\n",
    "            sentence, terms, label = sentence.cuda(), terms.cuda(), label.cuda()\n",
    "        \n",
    "        score = model(sentence, terms)\n",
    "        loss = criterion(score, label.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # meters update and visualize\n",
    "        if opt.use_cuda:\n",
    "            loss_meter.add(loss.data.cpu())\n",
    "            confusion_matrix.add(score.data.cpu(), label.data.cpu().squeeze())\n",
    "        else:\n",
    "            loss_meter.add(loss.data)\n",
    "            confusion_matrix.add(score.data, label.data.squeeze())\n",
    "        if total_step%opt.print_freq == 0:\n",
    "            vis.plot('loss', loss_meter.value()[0])\n",
    "            \"\"\"vis.log(\"score:{score},target:{label}\".format(\n",
    "                score = score,\n",
    "                label = label\n",
    "            ))\"\"\"\n",
    "        total_step += 1\n",
    "        \n",
    "    \n",
    "    # validate and visualize\n",
    "    val_cm, val_accuracy, class_equal_accuracy = val(model, test_dataloader)\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        model.save(acc=\"%.2f\"%val_accuracy)\n",
    "    \n",
    "    vis.plot('train_accuracy', 100.0*confusion_matrix.value().trace()/confusion_matrix.value().sum())\n",
    "    vis.plot('val_accuracy', val_accuracy)\n",
    "    vis.plot('val_class_equal_accuracy', class_equal_accuracy)\n",
    "    vis.plot('lr', lr)\n",
    "    vis.uplog(\"epoch:{},\\nlr:{},\\nval_cm:\\n{}\\n\".format(\n",
    "        epoch,lr,str(val_cm.value())\n",
    "    ).replace(\"\\n\", \"<br>\"))\n",
    "    \n",
    "    # update learning rate\n",
    "    if loss_meter.value()[0].item() >= previous_loss and lr>opt.lr_min:\n",
    "        lr = lr * opt.lr_decay\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    previous_loss = loss_meter.value()[0]\n",
    "    if epoch-1 % 100 == 0:\n",
    "        opt.lr_min *= 2/3\n",
    "    \"\"\"\n",
    "    lr = opt.lr * (np.cos(np.pi*(epoch/opt.max_epoch))+1)/2\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \"\"\"\n",
    "    if epoch%50 == 0:\n",
    "        print_attention(model, words, test_dataloader=test_dataloader)\n",
    "        print_attention(model, words, sentence_terms_label = (sentence, terms, label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
