{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import torch as t\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ../../ATAE-LSTM\\data\\Embedding.ipynb\n",
      "importing Jupyter notebook from ../../ATAE-LSTM\\models\\BasicModule.ipynb\n",
      "importing Jupyter notebook from ../../ATAE-LSTM\\data\\AspClas.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../ATAE-LSTM')\n",
    "import Ipynb_importer\n",
    "from config import opt\n",
    "from data.Embedding import Emb\n",
    "from models.BasicModule import BasicModule\n",
    "from data.AspClas import AspClas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATAE_LSTM(BasicModule):\n",
    "    def __init__(self, emb):\n",
    "        super(ATAE_LSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = emb._make_layer_()\n",
    "        \n",
    "        self.lstm = nn.LSTM(opt.hidden_size*2, opt.hidden_size, batch_first=True)\n",
    "        for k in self.lstm.state_dict().keys():\n",
    "            self.lstm.state_dict()[k].uniform_(-opt.epsilon, opt.epsilon)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.hidden=(\n",
    "            # 三个参数分别为 num_layers, batch_size, hidden_size\n",
    "            t.nn.Parameter(\n",
    "                t.Tensor(\n",
    "                    np.random.uniform(-opt.epsilon, opt.epsilon, opt.hidden_size)\n",
    "                ).view(1,1,opt.hidden_size)\n",
    "            ),\n",
    "            t.nn.Parameter(\n",
    "                t.Tensor(\n",
    "                    np.random.uniform(-opt.epsilon, opt.epsilon, opt.hidden_size)\n",
    "                ).view(1,1,opt.hidden_size)\n",
    "            )\n",
    "        )\"\"\"\n",
    "        self.h0 = t.nn.Parameter(t.Tensor(\n",
    "            np.random.uniform(-opt.epsilon, opt.epsilon, [1, opt.batch_size, opt.hidden_size])\n",
    "        ))\n",
    "        self.c0 = t.nn.Parameter(t.Tensor(\n",
    "            np.random.uniform(-opt.epsilon, opt.epsilon, [1, opt.batch_size, opt.hidden_size])\n",
    "        ))\n",
    "        \n",
    "        self.Wh = t.nn.Parameter(\n",
    "            t.Tensor(\n",
    "                np.random.uniform(-opt.epsilon, opt.epsilon, [opt.hidden_size, opt.hidden_size])\n",
    "            )\n",
    "        )\n",
    "        self.Wv = t.nn.Parameter(\n",
    "            t.Tensor(\n",
    "                np.random.uniform(-opt.epsilon, opt.epsilon, [opt.hidden_size, opt.hidden_size])\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.tanh = t.nn.Tanh()\n",
    "        \n",
    "        self.omega = t.nn.Parameter(\n",
    "            t.Tensor(\n",
    "                np.random.uniform(-opt.epsilon, opt.epsilon, opt.hidden_size*2)\n",
    "            ).view(opt.hidden_size*2, 1)\n",
    "        )\n",
    "        \n",
    "        self.Wp = t.nn.Parameter(\n",
    "            t.Tensor(\n",
    "                np.random.uniform(-opt.epsilon, opt.epsilon, [opt.hidden_size, opt.hidden_size])\n",
    "            )\n",
    "        )\n",
    "        self.Wx = t.nn.Parameter(\n",
    "            t.Tensor(\n",
    "                np.random.uniform(-opt.epsilon, opt.epsilon, [opt.hidden_size, opt.hidden_size])\n",
    "            )\n",
    "        )\n",
    "        self.relu = t.nn.LeakyReLU()\n",
    "        \n",
    "        self.lin = nn.Linear(opt.hidden_size, opt.classes)\n",
    "        for k in self.lin.state_dict().keys():\n",
    "            self.lin.state_dict()[k].uniform_(-opt.epsilon, opt.epsilon)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def forward(self, sentence, term):\n",
    "        \n",
    "        # sentence: [batch_size, max_seq_len]\n",
    "        # term: [batch_size, max_term_len]\n",
    "        \n",
    "        sentence_embeddings = self.embedding(sentence) # [batch_size, max_seq_len, hidden_size]\n",
    "        term_embeddings = []\n",
    "        for batch in range(term.size(0)):\n",
    "            valid_length = (term[batch, :] != 0).sum()\n",
    "            term_embeddings.append(self.embedding(term[batch, :valid_length]).mean(dim=0).unsqueeze(dim=0))\n",
    "        aspect_embeddings = t.cat(term_embeddings, dim=0) # [batch_size, hidden_size]\n",
    "        e1 = sentence_embeddings\n",
    "        e2 = aspect_embeddings.view(opt.batch_size,1,opt.hidden_size).expand(opt.batch_size,opt.max_seq_len,opt.hidden_size)\n",
    "        \n",
    "        # e1: [batch_size, max_seq_len, hidden_size]\n",
    "        # e2: [batch_size, max_seq_len, hidden_size]\n",
    "        \n",
    "        wv = t.cat((e1,e2),dim=-1)\n",
    "        # wv: [batch_size, max_seq_len, 2*hidden_size]\n",
    "        \n",
    "        out, (h, c) = self.lstm(wv, (self.h0, self.c0))\n",
    "        # e.g.\n",
    "        # out: [batch_size, max_seq_len, hidden_size]\n",
    "        # h: [batch_size, 1, hidden_size]\n",
    "        # c: [batch_size, 1, hidden_size]\n",
    "        \n",
    "        Wh_out = t.matmul(out, self.Wh)\n",
    "        # [batch_size, max_seq_len, hidden_size]\n",
    "        Wv_aspect = t.matmul(aspect_embeddings, self.Wv).view(opt.batch_size,1,opt.hidden_size).expand(opt.batch_size,opt.max_seq_len,opt.hidden_size)\n",
    "        # [batch_size, max_seq_len, hidden_size]\n",
    "        \n",
    "        vh = t.cat((Wh_out, Wv_aspect), dim=2)\n",
    "        # [batch_size, max_seq_len, 2*hidden_size]\n",
    "        \n",
    "        M = self.tanh(vh)\n",
    "        # [batch_size, max_seq_len, 2*hidden_size]\n",
    "        \n",
    "        alpha = nn.functional.softmax(t.matmul(M, self.omega), dim=1).view(opt.batch_size,1,opt.max_seq_len)\n",
    "        # [batch_size, 1, max_seq_len]\n",
    "        \n",
    "        r = t.matmul(alpha, out)\n",
    "        # [batch_size, 1, hidden_size]\n",
    "        \n",
    "        _h_ = self.relu(t.matmul(t.squeeze(r), self.Wp) + t.matmul(t.squeeze(h), self.Wx))\n",
    "        # [batch_size, hidden_size]\n",
    "        \n",
    "        y = nn.functional.softmax(self.lin(_h_), dim=1)\n",
    "        # [batch_size, num_classes]\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 100000/100000 [00:00<00:00, 118131.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 100000/100000 [00:14<00:00, 6975.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding : successfully input 100000 pretrained word embeddings while 0 failed\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    testDataset = AspClas(opt.test_data_root)\n",
    "    testDataLoader = DataLoader(testDataset, batch_size=opt.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "tensor([[110,  81, 111,  66,  32,   5,  66,  63, 112,  79,  65,  86,   5, 113,\n",
      "          89, 114,   3, 115, 116,  79,  66, 117,  20,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0],\n",
      "        [  2,   3,   4,   5,   6,   7,   8,   9,   3,  10,  11,  12,  13,   5,\n",
      "           3,  14,  11,  15,  16,  16,  16,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0],\n",
      "        [142,   3, 143,   5, 144,   7,  66,   9,   3,  63,   5,  32, 145,  89,\n",
      "         146,  79, 147, 148,  20,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0],\n",
      "        [  3,  21,  29,  11,  22,  23,  11,   3,  24,  30,   5,   3,  25,  26,\n",
      "          27,   3,  31,  11,  28,  20,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0]])\n",
      "tensor([[86,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [10,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [32,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [24, 30,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2]])\n",
      "tensor([[0.3342, 0.3332, 0.3326],\n",
      "        [0.3341, 0.3333, 0.3326],\n",
      "        [0.3342, 0.3332, 0.3326],\n",
      "        [0.3341, 0.3333, 0.3326]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    %pdb on\n",
    "    model = ATAE_LSTM(testDataset.emb)\n",
    "    sentence, terms, label = list(testDataLoader)[0]\n",
    "    print(sentence)\n",
    "    print(terms)\n",
    "    print(label)\n",
    "    y = model(sentence, terms)\n",
    "    print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
